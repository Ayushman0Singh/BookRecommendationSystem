{"cells": [{"cell_type": "markdown", "metadata": {"id": "view-in-github"}, "execution_count": null, "source": ["<a href=\"https://colab.research.google.com/github/Ayushman0Singh/BookRecommendationSystem/blob/main/BookRecommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Lm756o33cRpZ"}, "execution_count": null, "source": ["# **BOOK RECOMMENDATION**"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37192027_0.34824712971937877"}, "execution_count": null, "source": ["# Testing comming from DSlab"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "MdHEEq-PfNxa"}, "execution_count": null, "source": ["# Business Problem"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "92MpluFddPnA"}, "execution_count": null, "source": ["Online book reading and selling websites like Kindle and Goodreads compete against each other on many factors. One of those important factors is their book recommendation system. A book recommendation system is designed to recommend books of interest to the buyer.\n\n\nThe purpose of a book recommendation system is to predict buyer\u2019s interest and recommend books to them accordingly. A book recommendation system can take into account many parameters like book content and book quality by filtering user reviews.I will try to make a recommendation system for our given data set."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "J5L4ImAmp14J"}, "execution_count": null, "source": ["#importing necessary libraries \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport random\n# we will import libraries further as per need"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "mRjjIHfUf6XQ"}, "execution_count": null, "source": ["from google.colab import drive # mounting drive\ndrive.mount('/content/drive')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "aSYyTsCWg42p"}, "execution_count": null, "source": ["We have been given 3 data sets, Lets have a look at all the data provided to us and its properties"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "KBugC6Wnfq5G"}, "execution_count": null, "source": ["books = pd.read_csv(\"/content/drive/MyDrive/Almabetter/Capstone Projects/Unsupervised ML/Copy of Books.csv\")\nusers = pd.read_csv(\"/content/drive/MyDrive/Almabetter/Capstone Projects/Unsupervised ML/Copy of Users.csv\")\nratings = pd.read_csv(\"/content/drive/MyDrive/Almabetter/Capstone Projects/Unsupervised ML/Copy of Ratings.csv\")"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "gGqsJCshhoJK"}, "execution_count": null, "source": ["books.head(4) # checking the head and columns "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "X6d9sQGdYBvB"}, "execution_count": null, "source": ["users.head(5) #first look at the given data"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "7ldIYh6RYE8x"}, "execution_count": null, "source": ["ratings.head(5) # chcking the given data-sets"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Yai0x-7niEqP"}, "execution_count": null, "source": ["#dimensions of book dataframe\nbooks.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "eCHFFUt_hoO0"}, "execution_count": null, "source": ["#checking shape\nusers.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "3WLMUYi3BdxL"}, "execution_count": null, "source": ["ratings.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "8OlWxAguibFc"}, "execution_count": null, "source": ["# dimensions of the 3rd data set\nratings['Book-Rating'].value_counts()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5gPJ2qD5YUOn"}, "execution_count": null, "source": ["Many users have rated books 0"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Fc8RSPgtlpUl"}, "execution_count": null, "source": ["# Data Cleaning"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "WQ1nOcVSl1zA"}, "execution_count": null, "source": ["Before moving onto the data visualisation and EDA. First, lets make sure our data is ready to use.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "te9isOEpkiBn"}, "execution_count": null, "source": ["**Checking for null values data**"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "HF3i7OLiioUU"}, "execution_count": null, "source": ["# Books data-frame null values\nbooks.isnull().sum()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "vbgMsLNZYiXT"}, "execution_count": null, "source": ["# Users data-frame null values\nusers.isnull().sum()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "4lY6lWGJibIh"}, "execution_count": null, "source": ["#checking the ratings df for null values\nratings.isnull().sum()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4MA33LyCc0O9"}, "execution_count": null, "source": ["Whatever null values we have will be dealt with when do feature engineering and apply constraints. "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "O3zMVyjbi8g3"}, "execution_count": null, "source": ["# Exploratory Data Analysis"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "SYtIn2sJnV1v"}, "execution_count": null, "source": ["**Rating Distribution**"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "UXlm2bIVnbfk"}, "execution_count": null, "source": ["# show the distribution of rating\nplt.figure(figsize=(12, 6))\nsns.countplot(x='Book-Rating', data=ratings)\nplt.title('Rating Distribution')\nplt.xticks(rotation=90)\nplt.ylabel('Number of Books')\nplt.show()\nprint('Average rating recieved by all the apps is {}.'.format(ratings['Book-Rating'].mean()))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "pBhR8OuDn-7W"}, "execution_count": null, "source": ["Most of the ratings are zero\n\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "G1haZsUMzHA1"}, "execution_count": null, "source": ["**HYPOTHESIS**: Most of the users are between the age of 20 to 30 "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "6fJUH6zlyFaL"}, "execution_count": null, "source": ["# plotting the age of thee users \nplt.figure(figsize=(12, 6))\nsns.histplot(data=users['Age'], bins=np.arange(0,100,10))\nplt.title('Age Distribution\\n')\nplt.xlabel('Age')\nplt.ylabel('Count')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "_JJHpwhnY23g"}, "execution_count": null, "source": ["This verifies our hypothesis most of the users are in the age group of 20-30, followed by 30-40."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "OxaTJUiechxP"}, "execution_count": null, "source": ["# Data cleaning and feature Engineering"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "vGnaeO2N0vsm"}, "execution_count": null, "source": ["We will be using Two methods for our Books recommendation system, first we will use a memory Based colaborative model. Then we will also make a model based Colaboraative recomendation system. We will not be using content based algorithms for recommendation since we do not have enough indivisual features for users and the books. Users only has one extra feature. Moreover we might run into the cold-start problem. \n\nFor this exercise we will be using two models:\n\n**1) Memory based Collaborative filtering (using KNN)**\n\n**2) Model based Collaborative filtering (using SVD)**\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "I7fncK8vHfHg"}, "execution_count": null, "source": ["I tried to apply the above mentioned models to the full data set but I ran into memory problems. To solve these issues I applied a general threshold for our models. This also takes care of the cold start problem.\n\nTo reduce the data size we will try to apply certain constraints on the data frame. We can apply many different types of constraints to the dataset. These constraints include:\n\n1. **Popularity Threshold**: Minimum number of user-ratings for a book.\n2. **Active user Threshold**: Minimum number of books read for a unique user to be included in the recommendation system. \n3. **Regional Recommendation**: We will also recommend stuff regionally. The user will get recommendations from the users of the same location. We will do it for 1 region to not run into memory problems "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ha1akQEVDpHX"}, "execution_count": null, "source": ["**Active User Threshold**\n\n\nRemove users with less than 50 ratings(**inactive users**).\n\nTo to that, we will apply value_counts on user-id, each repetation of user means a new rating for a book by the same user. Then we will pick up users with atleast 50 repetions/ratings and filter them in our ratings data frame.\nThis also makes sure that all our users are consistent readers. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "pvpr3wigNcq9"}, "execution_count": null, "source": ["# checking the number of users and thier number ratings\ncounts1 = ratings['User-ID'].value_counts()\nprint(counts1)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "wlQPHxrbDrDZ"}, "execution_count": null, "source": ["#keeping users with more than 50 ratings \ncounts1_50 = counts1[counts1 >= 50].index # list of user-ids with more than 200 ratings\nratings = ratings[ratings['User-ID'].isin(counts1_50)]  # updating the whole data frame with only users with high ratings"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "o948KShHUIUD"}, "execution_count": null, "source": ["ratings"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "YozwbqegTvkl"}, "execution_count": null, "source": ["ratings['User-ID'].value_counts()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "77uLi0JdCNqi"}, "execution_count": null, "source": ["\nStarting from the original data set, we will be only looking at the popular books. In order to find out which books are popular, we combine books data with ratings data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "7WM3JhND_15f"}, "execution_count": null, "source": ["# merging rating with users\ncombine_book_rating = pd.merge(ratings, books,how = 'inner', on='ISBN') #merging two dataframes (inner join since we only want ifo of users with high rating)\ncolumns =['Book-Author',    'Year-Of-Publication',    'Publisher',    'Image-URL-S',    'Image-URL-M',    'Image-URL-L'] #list of unnecessary columns\ncombine_book_rating = combine_book_rating.drop(columns, axis=1) #droping those columns\ncombine_book_rating.head()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5Ti_-8LeghDy"}, "execution_count": null, "source": ["**Book Populatity Threshold**\n\nNext we will apply the constraint where we have a minimum number of ratings for a book to be considered in our recomendation system. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "okNYlGcvCh4e"}, "execution_count": null, "source": ["#collecting total rating counts\ncombine_book_rating = combine_book_rating.dropna(axis = 0, subset = ['Book-Title']) # clearing null/nan values from Book-Title\n#counting number of ratings for a book and renaming the columns appropiately\nbook_rating_Count = combine_book_rating.groupby(by = ['Book-Title'])['Book-Rating'].count().reset_index().rename(columns = {'Book-Rating': 'totalRatingCount'})\nbook_rating_Count.head()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Z-fdAKMr_1_y"}, "execution_count": null, "source": ["#combine with main data frame\n#use left join since we want rating count for all the books in combined_book_rating column\nrating_with_totalRatingCount = combine_book_rating.merge(book_rating_Count, left_on = 'Book-Title', right_on = 'Book-Title', how = 'left')\nrating_with_totalRatingCount.head()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "-wK1UtaU_2DL"}, "execution_count": null, "source": ["# looking at distribution of totalratingsCount\nrating_with_totalRatingCount.describe()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "MXG9gtoF_2Im"}, "execution_count": null, "source": ["#checking each quantiles of total ratings count closely to decide threshold\nprint(rating_with_totalRatingCount['totalRatingCount'].quantile(np.arange(0.1, 1, .05)))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "NP0jfb-kyaio"}, "execution_count": null, "source": ["We can see that there are many books with rating count more than ~30 this is just the distribution of total rating count. Lets, consider a threshold required count to be around 50. This will include many popular books which have been read by atleast 50 users. This will also make the opinion of users for a book more concrete, since there will be atleast 50 bad or good ratings for each of the books included."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "mfx7q7HVqUya"}, "execution_count": null, "source": ["popularity_threshold = 50\nrating_popular_book = rating_with_totalRatingCount[rating_with_totalRatingCount['totalRatingCount'] >= 50]\nrating_popular_book"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "2XsIhKMPEB73"}, "execution_count": null, "source": ["# Collaborative Filtering Using k-Nearest Neighbors (kNN) / Memory based Model"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "VBd7T8H_LG3P"}, "execution_count": null, "source": ["**Applying a Country/regional Threshold**\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "EORDd9QXK__N"}, "execution_count": null, "source": ["In order to improve computing speed, and not run into the \u201cMemoryError\u201d issue, I will limit our user data to those in the India and US. And then combine user data with the rating data and total rating count data.\n\nLets have a look at number of users from each of these countries."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "OCoQ26KwKgFs"}, "execution_count": null, "source": ["#mergeing the books+ratings data frame with the user info data frame\ncombined = rating_popular_book.merge(users, left_on = 'User-ID', right_on = 'User-ID', how = 'left')\n# users with location as Undia or US \nindia_us_user_rating = combined[combined['Location'].str.contains(\"india|usa\")]\nindia_us_user_rating= india_us_user_rating.drop('Age', axis=1)\nindia_us_user_rating.head()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Vt8YBtvTSmG7"}, "execution_count": null, "source": ["**Implementing KNN**"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "UKS7Do3ETtGU"}, "execution_count": null, "source": ["We use unsupervised algorithms with sklearn.neighbors. The algorithm we use to compute the nearest neighbors is \u201cbrute\u201d, and we specify \u201cmetric=cosine\u201d so that the algorithm will calculate the cosine similarity between rating vectors. Finally, we fit the model."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "C7Ud6WgE0idO"}, "execution_count": null, "source": ["from scipy.sparse import csr_matrix\nfrom sklearn.neighbors import NearestNeighbors\n\nindia_us_user_rating = india_us_user_rating.drop_duplicates(['User-ID', 'Book-Title'])  #dropping duplicates since we dont need it in the pivot matrix\nindia_us_user_rating_pivot = india_us_user_rating.pivot(index = 'Book-Title', columns = 'User-ID', values = 'Book-Rating').fillna(0) #filling nan values with zeroes\nplt.spy(india_us_user_rating_pivot) #checking the non-zero values in the matrix\nindia_us_user_rating_matrix = csr_matrix(india_us_user_rating_pivot.values)\nmodel_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\nmodel_knn.fit(india_us_user_rating_matrix)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "_tuZlNaJOToC"}, "execution_count": null, "source": ["plt.spy(india_us_user_rating_matrix)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "laL5dTFBbMOr"}, "execution_count": null, "source": ["india_us_user_rating_pivot.shape[1]"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Y59lz_Cf0irS"}, "execution_count": null, "source": ["#generating recommendations\nrandom_userbook_rating_index = np.random.choice(india_us_user_rating_pivot.shape[0]) #pick a random row from the pivot table\ndistances, indices = model_knn.kneighbors(india_us_user_rating_pivot.iloc[random_userbook_rating_index,].values.reshape(1, -1), n_neighbors = 6) # provide the row as features to the kNN\nfor i in range(0, len(distances.flatten())):   #loop through all the recomendations\n    if i == 0:  #selected book \n        print('Recommendations for {0}:\\n'.format(india_us_user_rating_pivot.index[random_userbook_rating_index])) \n    else:  \n        #recomendations based on cosine distance of the books from selected book\n        print('{0}: {1}, with distance of {2}:'.format(i,india_us_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i]))\n        "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "4aDE04rieMWH"}, "execution_count": null, "source": ["random_userbook_rating_index = np.random.choice(india_us_user_rating_pivot.shape[0]) #pick a random row from the pivot table\ndistances, indices = model_knn.kneighbors(india_us_user_rating_pivot.iloc[random_userbook_rating_index,].values.reshape(1, -1), n_neighbors = 6) # provide the row as features to the kNN\nfor i in range(0, len(distances.flatten())):\n    if i == 0:\n        print('Recommendations for {0}:\\n'.format(india_us_user_rating_pivot.index[random_userbook_rating_index]))\n    else:\n        print('{0}: {1}, with distance of {2}:'.format(i,india_us_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i]))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "t_tPqZdS_E1Z"}, "execution_count": null, "source": ["We have sucessfully impleted collaborative filering using KNN. The user-interacction matrix that we ended up with was quite small after all the constraints. Lets try to implement matrix factorisation **without any regional constraints**. \n\nThis time we will apply the **matrix-factorisation method/SVD** method for recommendations. We will assume some latent interactions between the users and items. Then we will try to come up with the user item interaction matrix by ourselves using SVD, in the process we would be filling the non-interacted items with a rating. We can rank these up for the best recommendations. "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ldKsXnFbLeIK"}, "execution_count": null, "source": ["# **Matrix Factorisation** / Model based approach"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "WdOWZScq_jHq"}, "execution_count": null, "source": ["#we will be using this matrix for the matrix factorisation method. \nrating_popular_book.drop(columns=['ISBN','totalRatingCount'], inplace = True) #removing useless columns"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "OlprfG8FBRpN"}, "execution_count": null, "source": ["rating_popular_book['User-ID'].value_counts() #checking user-id counts\n#since we are going to stratify 'User-ID' for our test train split, lets make sure all the user-ids have multiple instances"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "peRgDs9VBAj0"}, "execution_count": null, "source": ["#counting # of user-ids with 1 rating\nk = rating_popular_book['User-ID'].value_counts().reset_index() #creating a matrix with user-id counts\nk[k['User-ID'] == 1].shape[0]   #checking the number of User_ids which have only been repeated once"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "hlOXtUWMKchu"}, "execution_count": null, "source": ["#removing the rows with unique user_ID, since we need more than 1 to straify.\nrating_popular_book = rating_popular_book[rating_popular_book.duplicated(subset=[\"User-ID\"], keep=False)] "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "6LJg3TsCSzCc"}, "execution_count": null, "source": ["rating_popular_book.drop_duplicates(inplace = True)\nrating_popular_book.head(10)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "MHLbtoXH6jUK"}, "execution_count": null, "source": ["Since we will be developing an evaluation system for the recommendation system, We need to do a test train split. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "AsUm73pD65yj"}, "execution_count": null, "source": ["interactions_train_df, interactions_test_df = train_test_split(rating_popular_book, #spliting the user-item rating dataframe                                        \n                                   stratify=rating_popular_book.loc[:,'User-ID'],          #stratify using user-id column\n                                   test_size=0.20,                                   #using 20 percent data as test set\n                                   random_state=42)\n\nprint('# interactions on Train set: %d' % len(interactions_train_df))\nprint('# interactions on Test set: %d' % len(interactions_test_df))"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "sns4sIqEkEZ8"}, "execution_count": null, "source": ["#Creating a sparse pivot table with users in rows and items in columns\npivot_matrix = interactions_train_df.pivot_table(index='User-ID', columns='Book-Title', values='Book-Rating', aggfunc='sum').fillna(0) #agreegating any duplicate entries #filling nan values with 0\nplt.spy(pivot_matrix)  # visualising the sparse matrix\npivot_matrix"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "8vL5XrIakEdF"}, "execution_count": null, "source": ["from scipy.sparse.linalg import svds\n# The number of factors to factor the user-item matrix.\nNUMBER_OF_FACTORS_MF = 15\n\n#Performs matrix factorization of the original user item matrix\nU, sigma, Vt = svds(pivot_matrix, k = NUMBER_OF_FACTORS_MF)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "Gkj9qZIokEf6"}, "execution_count": null, "source": ["#checking shapes\nprint(U.shape)\nprint(sigma.shape)\nprint(Vt.shape)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "jGvZSLEZEGqi"}, "execution_count": null, "source": ["sigma"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "dNxc1v4REJEa"}, "execution_count": null, "source": ["Sigma here is an 1-d array with 15 elements we need to convert it to a diagonal matrix so that the matrix multiplication goes smoothly and the dimensions are correct."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "CPkDwT2EEkPv"}, "execution_count": null, "source": ["#making sigma a diagonal matrix\nsigma = np.diag(sigma)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "NNV0Cj31AFNM"}, "execution_count": null, "source": ["#reconstructing the original matrix without any zeros \nall_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \nall_user_predicted_ratings "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "snpDB6KaDyPh"}, "execution_count": null, "source": ["#checking if martix shape is same as the original matrix\nall_user_predicted_ratings.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "-MG0U370W02O"}, "execution_count": null, "source": ["# converting back to a dataframe\n# defining index as book-title\n# defining the columns as all the filtered user-ids\nprediction_df = pd.DataFrame(all_user_predicted_ratings.transpose(), index= pivot_matrix.columns, columns=pivot_matrix.index) "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "wttI5UYJXw6w"}, "execution_count": null, "source": ["prediction_df"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "28RIOQTwtHsS"}, "execution_count": null, "source": ["Now we have all the potential ratings for all the items and users. All thats left is to pick up these ratings of previously uninteracted items for each indivisual user and sort them in descending order to have a recommendation list for that user.\n\nWe also have to ignore the books which the user has already read and rated. Lets try it for a random user  638."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "7id9MVgjPfMR"}, "execution_count": null, "source": ["a = pivot_matrix.loc[638,:].reset_index() #making a data frame of ratings for an user\nunread_books = a[a[638] < 0.1]['Book-Title']      # filtering the books user has not read  #keeping a non-zero threshold to include all the uninteracted books\nunread_books  #books user hasnt interacted with yet "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "TNqj3_4xS5_X"}, "execution_count": null, "source": ["sorted_user_predictions = prediction_df[638].sort_values(ascending=False).reset_index().rename(columns = {638:'RecommendationStrength'})   #best recommendations for the user \n#making sure the recommendations are uninteracted books\nrecommendations = sorted_user_predictions[sorted_user_predictions['Book-Title'].isin(unread_books)].sort_values('RecommendationStrength', ascending = False)\nrecommendations.set_index('Book-Title', inplace=True) #setting books as index"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "chtaKKUsVNN9"}, "execution_count": null, "source": ["#extracting the top 5 recommendations. \nlist(recommendations.index[0:5])"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "HDlKKZ2YWYnH"}, "execution_count": null, "source": ["Now that we have successfully gotten recomendations for a user. Lets write a function to do the same for a chosen user. "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "vyEHCEesfjV1"}, "execution_count": null, "source": ["#fuction for top 5 recommendations for a user\ndef recommend_book(user_id):\n  a = pivot_matrix.loc[user_id,:].reset_index() #making a data frame of ratings for an user\n  unread_books = a[a[user_id] < 0.1]['Book-Title']      # filtering the books user has not read, uninteracted items are rated zero. \n  sorted_user_predictions = prediction_df[user_id].sort_values(ascending=False).reset_index().rename(columns = {user_id:'RecommendationStrength'})# getting best recommendations from the reconstructed matrix\n  recommendations = sorted_user_predictions[sorted_user_predictions['Book-Title'].isin(unread_books)].sort_values('RecommendationStrength', ascending = False) #making sure we are not recommending already interacted items. \n  recommendations.set_index('Book-Title', inplace=True) #setting index as book title\n  return list(recommendations.index[0:5]) #extracting the enquired information"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "WdT1wHxcYlC6"}, "execution_count": null, "source": ["recommend_book(638)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "gyuIzvzALyl8"}, "execution_count": null, "source": ["prediction_df.columns"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "4qW8I1fPceWx"}, "execution_count": null, "source": ["# Evaluation "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "_JwEmzo9hKR1"}, "execution_count": null, "source": ["#settinng User-Id as index in all our interactions data frame (full,train,test)\nfull_df_indexed = rating_popular_book.set_index('User-ID')\ninteractions_train_indexed_df = interactions_train_df.set_index('User-ID')\ninteractions_test_indexed_df = interactions_test_df.set_index('User-ID')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "3Kp_0TGHRHta"}, "execution_count": null, "source": ["# Function for getting the set of items which a user has interacted with\ndef get_items_interacted(user,interactions_df):\n  interacted_items = interactions_df.loc[user]['Book-Title']  #interacted-books for the user\n  return set(interacted_items)  #converting to set\n\n# Function for getting the set of items which a user has not interacted with in training set\ndef non_inter_items_train(user, seed = 42): \n  interacted_items = get_items_interacted(user, interactions_train_indexed_df)                            # taking all the interacted items from train set\n  all_items = set(interactions_train_df['Book-Title'])                                                    # all the items in train set\n  non_interacted_items = all_items - interacted_items                                                     # non-interacted items\n  random.seed(seed)                                                                                       # defining a random seed for consistency across users\n  non_interacted_items_sample = random.sample(non_interacted_items, 100)                                  # taking 100 non interacted items\n  return set(non_interacted_items_sample)                                                                 # set of the 100 non-interacted items from the train\n\n# Function to recommend the highest predicted rating content that the user hasn't seen yet\ndef recommend_items(user_id, items_to_ignore=[], topn=10):\n  sorted_user_predictions = prediction_df[user_id].sort_values(ascending=False).reset_index().rename(columns = {user_id:'RecommendationStrength'})\n  recommendations_df = sorted_user_predictions[~sorted_user_predictions['Book-Title'].isin(items_to_ignore)].sort_values('RecommendationStrength', ascending = False)\n  return recommendations_df\n\n# Function to verify whether a particular item_id was present in the set of top N recommended items\ndef top_n(book_name, recommended_items, topn):\n  try:\n      index = next(i for i, c in enumerate(recommended_items) if c == book_name)  #getting the item rank according to recommendation strength\n  except:\n      index = -1   #default value for index                                                            \n  hit = int(index in range(0, topn))            # hit is integer of true or false/ true when rank is in topn\n  return hit, index"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "igr27S9W8bWB"}, "execution_count": null, "source": ["We will be using the above defined functions to make our evaluator for the recommendation system. Now lets get to writing our recommedation systems"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "qb3GmuWPulRO"}, "execution_count": null, "source": ["### This evaluation method works as follows:\n\n* ### For each user\n    *  For each item the user has interacted in test set\n        *  Sample 100 other items the user has never interacted.   \n        *  Ask the recommender model to produce a ranked list of recommended items, from a set composed of one interacted item and the 100 non-interacted items\n        *  Compute the Top-N accuracy metrics for this user and interacted item from the recommendations ranked list\n* ### Aggregate the global Top-N accuracy metrics"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "5RUsrNyWZX9e"}, "execution_count": null, "source": ["# write a Function to evaluate the performance of model for each user\ndef evaluate_model_for_user(user_id):\n  #getting items in test set\n  interacted_values_testset = interactions_test_indexed_df.loc[user_id]\n  person_interacted_items_testset = set(interacted_values_testset['Book-Title'])\n  \n  interacted_items_count_testset = len(person_interacted_items_testset) \n  \n  # Getting a ranked recommendation list from the model for a given user\n  person_recs_df = recommend_items(user_id, items_to_ignore=get_items_interacted(user_id, interactions_train_indexed_df),topn=10000000000)\n  \n  hits_at_5_count = 0\n  hits_at_10_count = 0\n\n  # For each item the user has interacted in test set\n  for book in person_interacted_items_testset:\n\n    #getting a random sample of 100 people from train set and combing with our test set item\n    non_interacted_items_sample = non_inter_items_train(user_id)\n    items_to_filter_recs = non_interacted_items_sample.union(set([book]))\n\n    # Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items\n    valid_recs_df = person_recs_df[person_recs_df['Book-Title'].isin(items_to_filter_recs)]                    \n    valid_recs = valid_recs_df['Book-Title'].values\n            \n    # Verifying if the current interacted item is among the Top-N recommended items\n    hit_at_5, index_at_5 = top_n(book, valid_recs, 5)\n    hits_at_5_count += hit_at_5\n    hit_at_10, index_at_10 = top_n(book, valid_recs, 10)\n    hits_at_10_count += hit_at_10\n\n  # Recall is the rate of the interacted items that are ranked among the Top-N recommended items\n  recall_at_5 = hits_at_5_count / float(interacted_items_count_testset)\n  recall_at_10 = hits_at_10_count / float(interacted_items_count_testset)\n\n  user_metrics = {'hits@5_count':hits_at_5_count, \n                          'hits@10_count':hits_at_10_count, \n                          'interacted_count': interacted_items_count_testset,\n                          'recall@5': recall_at_5,\n                          'recall@10': recall_at_10}\n  return user_metrics\n\n\n# Function to evaluate the performance of model for all users( overall performance )\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "-K7HzRT1J48q"}, "execution_count": null, "source": ["evaluate_model_for_user(28204)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "FYFrLlIFZX_8"}, "execution_count": null, "source": ["# storing metrics of all the users in the test-set       \npeople_metrics = [] \nfor idx, person_id in enumerate(list(interactions_test_indexed_df.index.unique().values)):    \n  person_metrics = evaluate_model_for_user(person_id)  \n  person_metrics['_person_id'] = person_id\n  people_metrics.append(person_metrics)\n            \nprint('%d users processed' % idx)\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "oroPGfM1PMJs"}, "execution_count": null, "source": ["# Evaluating global metrics \ndetailed_results_df = pd.DataFrame(people_metrics).sort_values('interacted_count', ascending=False)\nglobal_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['interacted_count'].sum())\nglobal_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['interacted_count'].sum())\n\nglobal_metrics = {'recall@5': global_recall_at_5,'recall@10': global_recall_at_10} \n\nprint(global_metrics)                    "], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "qU0JHO_7VPPY"}, "execution_count": null, "source": ["# Conclusion"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "jf3318CGY4Bq"}, "execution_count": null, "source": ["\n\nThis brings us to the end of our exercise.\nWe running a recommendation system with the whole data set but we kept running into memory problems. \n\nSo, I put some constraints on the data set and tried collaborative filtering for our recommendations. We used both memory-based and model-based approaches. \nI also developed an top_n evaluation system for my model-based collaborative filtering approach.\n\nThanks for reading!"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}